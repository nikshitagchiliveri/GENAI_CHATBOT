{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikshitagchiliveri/GENAI_CHATBOT/blob/main/Session_3_Build_your_own_AI_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversational UI Chatbot App with Gemini, LangChain and Streamlit\n",
        "\n",
        "Here we will build a advanced Conversational UI-based chatbot using LangChain and Streamlit with the following features:\n",
        "\n",
        "- Custom Landing Page\n",
        "- Conversational memory"
      ],
      "metadata": {
        "id": "xPRVq3e03c1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install App and LLM dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.1.12 -q\n",
        "!pip install langchain-google-genai==0.0.7 -q\n",
        "!pip install langchain-community==0.0.29 -q\n",
        "!pip install streamlit==1.32.2 -q\n",
        "!pip install pyngrok==7.1.5 -q\n",
        "!pip install google-generativeai>=0.3.2 -q"
      ],
      "metadata": {
        "id": "2evPp14fy258",
        "outputId": "1d8c49f5-ca42-4e3d-b958-9f934554f9a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m971.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.1/303.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-cloud-bigquery 3.31.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.9/146.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.7/598.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "google-cloud-bigquery 3.31.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Gemini API Credentials\n",
        "\n",
        "Here we load it from a file so we don't explore the credentials on the internet by mistake"
      ],
      "metadata": {
        "id": "CiwGjVWK4q6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "kDe44J0N0NcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Streamlit UI without Memory\n",
        "\n",
        "This simple version shows how to:\n",
        "\n",
        "1. Create a basic Streamlit interface\n",
        "2. Connect directly to Gemini\n",
        "3. Process basic Q&A without memory"
      ],
      "metadata": {
        "id": "-Eio-LoBKnzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile basic_app.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "st.title(\"Basic AI Assistant (No Memory)\")\n",
        "\n",
        "# Initialize the LLM\n",
        "gemini = ChatGoogleGenerativeAI(model='gemini-2.0-flash-001',\n",
        "                               temperature=0.1)\n",
        "\n",
        "# Simple input/output\n",
        "if user_input := st.chat_input(\"Ask a question\"):\n",
        "    st.chat_message(\"human\").write(user_input)\n",
        "\n",
        "    with st.chat_message(\"ai\"):\n",
        "        response = gemini.invoke(user_input)\n",
        "        st.write(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBfzThOWKu42",
        "outputId": "a0c160ed-1315-4dfe-c3d0-208c09feb119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing basic_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start the app"
      ],
      "metadata": {
        "id": "6naokMhCMiPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run basic_app.py --server.port=8989 &>./logs.txt &"
      ],
      "metadata": {
        "id": "UZGYWmcfMiPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "ngrok.set_auth_token(userdata.get('NGROK_API_KEY'))\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0729e54-76d4-436d-ba4c-1c11adcb9a63",
        "id": "WR65GrUEMrcD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App: https://1160-34-106-180-235.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove running app processes"
      ],
      "metadata": {
        "id": "I_k4wNzQMrcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "NJhCkMhyMrcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baaf30dc-765d-409d-b0a1-f7d7a31bb2b0",
        "id": "LT8xZ404MrcH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        1645       1  1 09:17 ?        00:00:04 /usr/bin/python3 /usr/local/bin/streamlit run basic_app.py --server.port=8989\n",
            "root        2660    1097  0 09:21 ?        00:00:00 /bin/bash -c ps -ef | grep streamlit\n",
            "root        2662    2660  0 09:21 ?        00:00:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo kill -9 50727"
      ],
      "metadata": {
        "id": "kIjWRCybMrcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc8886f-a040-432c-a5d9-5576e61cdaf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kill: (50727): No such process\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Showing Messages with Manual Memory\n",
        "\n",
        "This demonstrates:\n",
        "\n",
        "1. How to manually implement memory in Streamlit\n",
        "2. The challenge of formatting context for the LLM\n",
        "3. Why specialized tools like LangChain help"
      ],
      "metadata": {
        "id": "-svMcbHaNLpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile manual_app.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "st.title(\"AI Assistant with Manual Memory\")\n",
        "\n",
        "# Initialize the LLM\n",
        "gemini = ChatGoogleGenerativeAI(model='gemini-2.0-flash-001',\n",
        "                               temperature=0.1)\n",
        "\n",
        "# Initialize session state for memory\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display existing messages\n",
        "for message in st.session_state.messages:\n",
        "    st.chat_message(message[\"role\"]).write(message[\"content\"])\n",
        "\n",
        "# Process new input\n",
        "if user_input := st.chat_input(\"Ask a question\"):\n",
        "    # Add user message to history\n",
        "    st.session_state.messages.append({\"role\": \"human\", \"content\": user_input})\n",
        "    st.chat_message(\"human\").write(user_input)\n",
        "\n",
        "    # Manually create a context string from history\n",
        "    context = \"Previous conversation:\\n\"\n",
        "    for msg in st.session_state.messages:\n",
        "        context += f\"{msg['role']}: {msg['content']}\\n\"\n",
        "\n",
        "    with st.chat_message(\"ai\"):\n",
        "        # Send context + new question\n",
        "        full_prompt = context + \"\\nPlease respond to the last question.\"\n",
        "        response = gemini.invoke(full_prompt)\n",
        "        st.write(response.content)\n",
        "\n",
        "        # Add AI response to history\n",
        "        st.session_state.messages.append({\"role\": \"ai\", \"content\": response.content})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgYHo7QwNULa",
        "outputId": "26e372a6-c483-4fc0-f2b7-9884aaa42c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing manual_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run manual_app.py --server.port=8989 &>./logs.txt &"
      ],
      "metadata": {
        "id": "9bqZgBFiNg_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "ngrok.set_auth_token(userdata.get('NGROK_API_KEY'))\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ce9f5a-afff-4b58-cc2b-fb633c591d65",
        "id": "ZCA-ShfgNg_e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App: https://8394-34-106-180-235.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "lX0v5KEeNg_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e9490f-9960-4137-cff3-40bbf273f035",
        "id": "TnlrPbd6Ng_g"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        1645       1  0 09:17 ?        00:00:06 /usr/bin/python3 /usr/local/bin/streamlit run basic_app.py --server.port=8989\n",
            "root        6582    1097  0 09:37 ?        00:00:00 /bin/bash -c ps -ef | grep streamlit\n",
            "root        6584    6582  0 09:37 ?        00:00:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo kill -9 1645"
      ],
      "metadata": {
        "id": "e46FJ2B9Ng_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Full LangChain Integration"
      ],
      "metadata": {
        "id": "RCMshwB1U9iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile langchain_app.py\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from operator import itemgetter\n",
        "import streamlit as st\n",
        "\n",
        "# Customize initial app landing page\n",
        "st.set_page_config(page_title=\"AI Assistant\", page_icon=\"🤖\")\n",
        "st.title(\"Welcome I am AI Assistant 🤖\")\n",
        "\n",
        "# Load a connection to Gemini LLM\n",
        "gemini = ChatGoogleGenerativeAI(model='gemini-2.0-flash-001',\n",
        "                               temperature=0.1,\n",
        "                               convert_system_message_to_human=True)\n",
        "\n",
        "# Add a basic system prompt for LLM behavior\n",
        "SYS_PROMPT = \"\"\"\n",
        "              Act as a helpful assistant and answer questions to the best of your ability.\n",
        "              Do not make up answers.\n",
        "              \"\"\"\n",
        "\n",
        "# Create a prompt template for langchain to use history to answer user prompts\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "  [\n",
        "    (\"system\", SYS_PROMPT),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "  ]\n",
        ")\n",
        "\n",
        "# Create a basic llm chain\n",
        "llm_chain = (\n",
        "  prompt\n",
        "  |\n",
        "  gemini\n",
        ")\n",
        "\n",
        "# Store conversation history in Streamlit session state\n",
        "streamlit_msg_history = StreamlitChatMessageHistory()\n",
        "\n",
        "# Create a conversation chain\n",
        "conversation_chain = RunnableWithMessageHistory(\n",
        "  llm_chain,\n",
        "  lambda session_id: streamlit_msg_history,  # Accesses memory\n",
        "  input_messages_key=\"input\",\n",
        "  history_messages_key=\"history\",\n",
        ")\n",
        "\n",
        "# Render current messages from StreamlitChatMessageHistory\n",
        "for msg in streamlit_msg_history.messages:\n",
        "  st.chat_message(msg.type).write(msg.content)\n",
        "\n",
        "# If user inputs a new prompt, display it and show the response\n",
        "if user_prompt := st.chat_input():\n",
        "  st.chat_message(\"human\").write(user_prompt)\n",
        "  # This is where response from the LLM is shown\n",
        "  with st.chat_message(\"ai\"):\n",
        "    config = {\"configurable\": {\"session_id\": \"any\"}}\n",
        "    # Get llm response\n",
        "    response = conversation_chain.invoke({\"input\": user_prompt}, config)\n",
        "    st.markdown(response.content) # Display response directly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLpP0t_9ojVI",
        "outputId": "78500c63-a54b-4e0a-d6f7-fbf6bf3cc2ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting langchain_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start the app"
      ],
      "metadata": {
        "id": "8de1tM6FVLsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run langchain_app.py --server.port=8989 &>./logs.txt &"
      ],
      "metadata": {
        "id": "Za_TAI2RkPI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "ngrok.set_auth_token(userdata.get('NGROK_API_KEY'))\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "id": "FrVhyQVirAqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18b974e-c38a-4a96-f1c7-648a57f7e103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App: https://bd40-34-106-180-235.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove running app processes"
      ],
      "metadata": {
        "id": "2Q3yFB_jsgC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "pG7Abg_LrAw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep streamlit"
      ],
      "metadata": {
        "id": "x2VA4ZtCkPNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25864dc2-556f-4c4f-e583-4b91eafa7cac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        6632       1  0 09:37 ?        00:00:04 /usr/bin/python3 /usr/local/bin/streamlit run langchain_app.py --server.port=8989\n",
            "root        9281    1097  0 09:48 ?        00:00:00 /bin/bash -c ps -ef | grep streamlit\n",
            "root        9283    9281  0 09:48 ?        00:00:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo kill -9 6632"
      ],
      "metadata": {
        "id": "_WxGAxGHkPLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MlBQh5fdkPPY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}